# Group 137
# Mitchell Danks        S320289 
# Joshua Hall           S377706 
# Brooke Stokes         S364762 
# Repositorie Link      https://github.com/MitchellDanks/HIT137CAS080.git

#--------------------------------------------------------------------------------------
#to load error library, operating sytem and csv file tools
import os
import warnings
import csv

#ignore update versions messages
warnings.filterwarnings('ignore', message='possible set union')
warnings.filterwarnings('ignore', message='clean_up_tokenization_spaces')

#consider creating a virtual environment to install libraries as recommended

#pip install or python3<FILE_NAME>.py install
import subprocess

#need PyTorch first to run scispacY
subprocess.run(['pip', 'install', 'torch'])

#access the libraries call the cmd prompt to download required libraries from the internet
subprocess.run(['pip', 'install', 'https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz'])
subprocess.run(['pip', 'install', 'https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz'])
subprocess.run(['pip', 'install', 'git+https://github.com/huggingface/transformers'])

#validate loop to check the installation of the libraries
try:
    #Pytorch is needed to run transformers libraries
    import torch
    import transformers
    #load Biobert from hugging face and tokenizer for pipeline
    from transformers import AutoTokenizer, AutoModelForTokenClassification
    from transformers import pipeline
    model_name = 'dmis-lab/biobert-base-cased-v1.1'
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForTokenClassification.from_pretrained(model_name)
    ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)

    #Message to validate loading  
    print("----------------------------LOADED----------------------------------")  
    print("CONGRATULATIONS all libraries were installed and successfully LOADED")

except ImportError as e:
    print(f'Error in loading library: {e}')
except OSError as e:
    print(f'Check your operating system: {e}')
except Exception as e:
    print(f'Surprise, an unexpected error has occurred: {e}')

#-------------------------------------------------------------------------------------------------------------------------------
#Task 4.1_Use spacy to assess .txt
#Load the spacy toolset/library
import spacy

# Load the models for natural language processing NLP training with SpaCy in biomedical knowledge and drugs and diseases
sci_sm_nlp = spacy.load("en_core_sci_sm")
bc5cdr_nlp = spacy.load("en_ner_bc5cdr_md")

#set directory of .txt file acquired in Q1_Task1
output_txt ="Question 1/A2_Q1.1.txt"

#Open file to append chunks to CSV as they are processed to save memory and data loss
output_spacy_processed = "Question 1/A2_Q1.4_appending.csv"

#Load large text file in chunks
def load_large_text_file_in_chunks(file_path, chunk_size=500):
    with open(file_path, 'r', encoding='utf-8') as file:
        while True:
            chunk = file.read(chunk_size)
            if not chunk:
                break
            yield chunk

#SpaCy extraction function
def extract_entities_spacy(chunk):
    doc = bc5cdr_nlp(chunk)
    diseases_spacy = [ent.text for ent in doc.ents if ent.label_ == "DISEASE"]
    drugs_spacy = [ent.text for ent in doc.ents if ent.label_ == "CHEMICAL"]
    return diseases_spacy, drugs_spacy

#BioBERT extraction function
def extract_entities_biobert(chunk):
    ner_results = ner_pipeline(chunk)
    diseases_bert = [entity['word'] for entity in ner_results if "DISEASE" in entity['entity']]
    drugs_bert = [entity['word'] for entity in ner_results if "DRUG" in entity['entity']]
    return diseases_bert, drugs_bert

#Process the chunks with SpaCy and BioBERT, combining the actions
def process_large_file_append_to_csv(input_file, output_csv, chunk_size=500):
    # Keep the CSV file open in append mode before processing begins
    with open(output_csv, 'a', newline='') as csv_file:
        writer = csv.writer(csv_file)

        #Write the header into the CSV file
        writer.writerow(['Spacy Vs BioBERT Library Pass of DISEASES and DRUGS'])
        writer.writerow(['DISEASES_spacy', 'DRUGS_spacy', 'DISEASES_BERT', 'DRUGS_BERT'])

        #Process file in chunks
        for chunk in load_large_text_file_in_chunks(input_file, chunk_size):
            # Pass over looking for diseases and drugs using SpaCy
            diseases_spacy, drugs_spacy = extract_entities_spacy(chunk)
            # Pass over looking for diseases and drugs using BioBERT
            diseases_bert, drugs_bert = extract_entities_biobert(chunk)

            #Sort and write information to append the results for each chunk to the CSV file
            for i in range(max(len(diseases_spacy), len(drugs_spacy), len(diseases_bert), len(drugs_bert))):
                writer.writerow([diseases_spacy[i] if i < len(diseases_spacy) else "",
                                 drugs_spacy[i] if i < len(drugs_spacy) else "",
                                 diseases_bert[i] if i < len(diseases_bert) else "",
                                 drugs_bert[i] if i < len(drugs_bert) else ""])

#Call the function to repeat and process
process_large_file_append_to_csv(output_txt, output_spacy_processed, chunk_size=500)

#Notify user that file is being updated
print(f'Your file is being updated at {output_spacy_processed}')
